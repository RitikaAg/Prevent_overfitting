{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import *\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.optimizers import Adam\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, StratifiedKFold \nfrom sklearn.metrics import roc_auc_score, mean_squared_error, log_loss\nimport os\nprint(os.listdir(\"../input\"))","execution_count":10,"outputs":[{"output_type":"stream","text":"['test.csv', 'train.csv', 'sample_submission.csv']\n","name":"stdout"}]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/train.csv')\ndf.head()\n# features selected by RFECV with lasso\nfeatures = ['16', '33', '43', '45', '52', '63', '65', '73', '90', '91', '117', '133', '134', '149', '189', '199', '217', '237', '258', '295']","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df[features].values\ny = df.values[:,1]\nprint(X.shape, y.shape)","execution_count":12,"outputs":[{"output_type":"stream","text":"(250, 20) (250,)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def simple_model(input_shape):\n    \"\"\"\n    define neural network model\n    \"\"\"\n    inp = Input(shape=(input_shape[1],))\n    x = Dense(3, activation='sigmoid')(inp)\n    # only keep this layer, then the model becomes logistic regression\n    x = Dense(1, activation='sigmoid')(x)\n    \n    model = Model(inputs=inp, outputs=x)\n    model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.005), metrics=['accuracy'])\n    return model","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"N_SPLITS = 10\nsplits = list(StratifiedKFold(n_splits=N_SPLITS, shuffle=True).split(X, y))\npreds_val = []\ny_val = []\nbest_models = []\n\nfor idx, (train_idx, val_idx) in enumerate(splits):\n    print(\"Beginning fold {}\".format(idx+1))\n    X_train, y_train, X_val, y_val = X[train_idx], y[train_idx], X[val_idx], y[val_idx]\n    model = simple_model(X_train.shape)\n    cb = ModelCheckpoint('weights.h5', monitor='val_acc', mode='max', save_best_only=True, save_weights_only=True)\n    model.fit(X_train, y_train, epochs=200, validation_data=(X_val, y_val), callbacks=[cb], verbose=0)\n    model.load_weights('weights.h5')\n    score = roc_auc_score(y_val, model.predict(X_val))\n    print((model, score))\n    best_models.append((model, score))","execution_count":14,"outputs":[{"output_type":"stream","text":"Beginning fold 1\n(<keras.engine.training.Model object at 0x7fa2cf56ee10>, 0.9027777777777779)\nBeginning fold 2\n(<keras.engine.training.Model object at 0x7fa2cf1437b8>, 0.8819444444444445)\nBeginning fold 3\n(<keras.engine.training.Model object at 0x7fa2ced8fe48>, 0.9375)\nBeginning fold 4\n(<keras.engine.training.Model object at 0x7fa2ce9fbd68>, 0.9791666666666667)\nBeginning fold 5\n(<keras.engine.training.Model object at 0x7fa2ce677dd8>, 0.5625)\nBeginning fold 6\n(<keras.engine.training.Model object at 0x7fa2ce2457b8>, 0.9027777777777778)\nBeginning fold 7\n(<keras.engine.training.Model object at 0x7fa2cdeb46a0>, 0.8958333333333333)\nBeginning fold 8\n(<keras.engine.training.Model object at 0x7fa2cdafef60>, 0.9444444444444444)\nBeginning fold 9\n(<keras.engine.training.Model object at 0x7fa2cd6e8be0>, 0.9652777777777777)\nBeginning fold 10\n(<keras.engine.training.Model object at 0x7fa2cd2ce400>, 0.9652777777777777)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = pd.read_csv('../input/test.csv')\nprint(len(df_test))\ndf_test.head()\n","execution_count":15,"outputs":[{"output_type":"stream","text":"19750\n","name":"stdout"},{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"    id      0      1      2      3  ...      295    296    297    298    299\n0  250  0.500 -1.033 -1.595  0.309  ...    2.132  0.609 -0.104  0.312  0.979\n1  251  0.776  0.914 -0.494  1.347  ...   -1.133 -3.138  0.281 -0.625 -0.761\n2  252  1.750  0.509 -0.057  0.835  ...    0.701  0.976  0.135 -1.327  2.463\n3  253 -0.556 -1.855 -0.682  0.578  ...    0.916  2.411  1.053 -1.601 -1.529\n4  254  0.754 -0.245  1.173 -1.623  ...    0.322 -0.068 -0.156 -1.153  0.825\n\n[5 rows x 301 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n      <th>12</th>\n      <th>13</th>\n      <th>14</th>\n      <th>15</th>\n      <th>16</th>\n      <th>17</th>\n      <th>18</th>\n      <th>19</th>\n      <th>20</th>\n      <th>21</th>\n      <th>22</th>\n      <th>23</th>\n      <th>24</th>\n      <th>25</th>\n      <th>26</th>\n      <th>27</th>\n      <th>28</th>\n      <th>29</th>\n      <th>30</th>\n      <th>31</th>\n      <th>32</th>\n      <th>33</th>\n      <th>34</th>\n      <th>35</th>\n      <th>36</th>\n      <th>37</th>\n      <th>38</th>\n      <th>...</th>\n      <th>260</th>\n      <th>261</th>\n      <th>262</th>\n      <th>263</th>\n      <th>264</th>\n      <th>265</th>\n      <th>266</th>\n      <th>267</th>\n      <th>268</th>\n      <th>269</th>\n      <th>270</th>\n      <th>271</th>\n      <th>272</th>\n      <th>273</th>\n      <th>274</th>\n      <th>275</th>\n      <th>276</th>\n      <th>277</th>\n      <th>278</th>\n      <th>279</th>\n      <th>280</th>\n      <th>281</th>\n      <th>282</th>\n      <th>283</th>\n      <th>284</th>\n      <th>285</th>\n      <th>286</th>\n      <th>287</th>\n      <th>288</th>\n      <th>289</th>\n      <th>290</th>\n      <th>291</th>\n      <th>292</th>\n      <th>293</th>\n      <th>294</th>\n      <th>295</th>\n      <th>296</th>\n      <th>297</th>\n      <th>298</th>\n      <th>299</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>250</td>\n      <td>0.500</td>\n      <td>-1.033</td>\n      <td>-1.595</td>\n      <td>0.309</td>\n      <td>-0.714</td>\n      <td>0.502</td>\n      <td>0.535</td>\n      <td>-0.129</td>\n      <td>-0.687</td>\n      <td>1.291</td>\n      <td>0.507</td>\n      <td>-0.317</td>\n      <td>1.848</td>\n      <td>-0.232</td>\n      <td>-0.340</td>\n      <td>-0.051</td>\n      <td>0.804</td>\n      <td>0.764</td>\n      <td>1.860</td>\n      <td>0.262</td>\n      <td>1.112</td>\n      <td>-0.491</td>\n      <td>-1.039</td>\n      <td>-0.492</td>\n      <td>0.183</td>\n      <td>-0.671</td>\n      <td>-1.313</td>\n      <td>0.149</td>\n      <td>0.244</td>\n      <td>1.072</td>\n      <td>-1.003</td>\n      <td>0.832</td>\n      <td>-1.075</td>\n      <td>1.988</td>\n      <td>1.201</td>\n      <td>-2.065</td>\n      <td>-0.826</td>\n      <td>-0.016</td>\n      <td>0.490</td>\n      <td>...</td>\n      <td>0.824</td>\n      <td>0.928</td>\n      <td>1.372</td>\n      <td>1.505</td>\n      <td>0.645</td>\n      <td>0.641</td>\n      <td>-1.132</td>\n      <td>1.009</td>\n      <td>0.998</td>\n      <td>0.210</td>\n      <td>-1.634</td>\n      <td>1.046</td>\n      <td>0.114</td>\n      <td>-0.806</td>\n      <td>0.301</td>\n      <td>0.145</td>\n      <td>-0.684</td>\n      <td>0.794</td>\n      <td>-0.290</td>\n      <td>-1.688</td>\n      <td>0.313</td>\n      <td>1.140</td>\n      <td>0.447</td>\n      <td>-0.616</td>\n      <td>1.294</td>\n      <td>0.785</td>\n      <td>0.453</td>\n      <td>1.550</td>\n      <td>-0.866</td>\n      <td>1.007</td>\n      <td>-0.088</td>\n      <td>-2.628</td>\n      <td>-0.845</td>\n      <td>2.078</td>\n      <td>-0.277</td>\n      <td>2.132</td>\n      <td>0.609</td>\n      <td>-0.104</td>\n      <td>0.312</td>\n      <td>0.979</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>251</td>\n      <td>0.776</td>\n      <td>0.914</td>\n      <td>-0.494</td>\n      <td>1.347</td>\n      <td>-0.867</td>\n      <td>0.480</td>\n      <td>0.578</td>\n      <td>-0.313</td>\n      <td>0.203</td>\n      <td>1.356</td>\n      <td>-1.086</td>\n      <td>0.322</td>\n      <td>0.876</td>\n      <td>-0.563</td>\n      <td>-1.394</td>\n      <td>0.385</td>\n      <td>1.891</td>\n      <td>-2.107</td>\n      <td>-0.636</td>\n      <td>-0.055</td>\n      <td>-0.843</td>\n      <td>0.041</td>\n      <td>0.253</td>\n      <td>0.557</td>\n      <td>0.475</td>\n      <td>-0.839</td>\n      <td>-1.146</td>\n      <td>1.210</td>\n      <td>1.427</td>\n      <td>0.347</td>\n      <td>1.077</td>\n      <td>-0.194</td>\n      <td>0.323</td>\n      <td>0.543</td>\n      <td>0.894</td>\n      <td>1.190</td>\n      <td>0.342</td>\n      <td>-0.858</td>\n      <td>0.756</td>\n      <td>...</td>\n      <td>-1.791</td>\n      <td>0.122</td>\n      <td>-0.669</td>\n      <td>-1.558</td>\n      <td>-0.244</td>\n      <td>2.583</td>\n      <td>-0.829</td>\n      <td>0.133</td>\n      <td>-2.746</td>\n      <td>0.341</td>\n      <td>-1.145</td>\n      <td>0.492</td>\n      <td>0.437</td>\n      <td>-0.628</td>\n      <td>0.271</td>\n      <td>2.639</td>\n      <td>0.481</td>\n      <td>-0.687</td>\n      <td>1.017</td>\n      <td>1.648</td>\n      <td>-1.272</td>\n      <td>-0.797</td>\n      <td>-0.870</td>\n      <td>-1.582</td>\n      <td>-1.987</td>\n      <td>-0.052</td>\n      <td>-0.194</td>\n      <td>0.539</td>\n      <td>-1.788</td>\n      <td>-0.433</td>\n      <td>-0.683</td>\n      <td>-0.066</td>\n      <td>0.025</td>\n      <td>0.606</td>\n      <td>-0.353</td>\n      <td>-1.133</td>\n      <td>-3.138</td>\n      <td>0.281</td>\n      <td>-0.625</td>\n      <td>-0.761</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>252</td>\n      <td>1.750</td>\n      <td>0.509</td>\n      <td>-0.057</td>\n      <td>0.835</td>\n      <td>-0.476</td>\n      <td>1.428</td>\n      <td>-0.701</td>\n      <td>-2.009</td>\n      <td>-1.378</td>\n      <td>0.167</td>\n      <td>-0.132</td>\n      <td>0.459</td>\n      <td>-0.341</td>\n      <td>0.014</td>\n      <td>0.184</td>\n      <td>-0.460</td>\n      <td>-0.991</td>\n      <td>-1.039</td>\n      <td>0.992</td>\n      <td>1.036</td>\n      <td>1.552</td>\n      <td>-0.830</td>\n      <td>1.374</td>\n      <td>-0.914</td>\n      <td>0.427</td>\n      <td>0.027</td>\n      <td>0.327</td>\n      <td>1.117</td>\n      <td>0.871</td>\n      <td>-2.556</td>\n      <td>-0.036</td>\n      <td>-0.081</td>\n      <td>0.744</td>\n      <td>-1.191</td>\n      <td>-1.784</td>\n      <td>0.239</td>\n      <td>0.500</td>\n      <td>0.437</td>\n      <td>0.746</td>\n      <td>...</td>\n      <td>-1.167</td>\n      <td>1.009</td>\n      <td>-0.180</td>\n      <td>-0.683</td>\n      <td>-1.383</td>\n      <td>1.020</td>\n      <td>0.268</td>\n      <td>-1.558</td>\n      <td>0.620</td>\n      <td>-0.489</td>\n      <td>-2.090</td>\n      <td>-0.977</td>\n      <td>1.672</td>\n      <td>-0.655</td>\n      <td>-0.801</td>\n      <td>-1.846</td>\n      <td>0.761</td>\n      <td>-0.846</td>\n      <td>0.181</td>\n      <td>0.962</td>\n      <td>-0.611</td>\n      <td>1.450</td>\n      <td>0.021</td>\n      <td>0.320</td>\n      <td>-0.951</td>\n      <td>-2.662</td>\n      <td>0.761</td>\n      <td>-0.665</td>\n      <td>-0.619</td>\n      <td>-0.645</td>\n      <td>-0.094</td>\n      <td>0.351</td>\n      <td>-0.607</td>\n      <td>-0.737</td>\n      <td>-0.031</td>\n      <td>0.701</td>\n      <td>0.976</td>\n      <td>0.135</td>\n      <td>-1.327</td>\n      <td>2.463</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>253</td>\n      <td>-0.556</td>\n      <td>-1.855</td>\n      <td>-0.682</td>\n      <td>0.578</td>\n      <td>1.592</td>\n      <td>0.512</td>\n      <td>-1.419</td>\n      <td>0.722</td>\n      <td>0.511</td>\n      <td>0.567</td>\n      <td>0.356</td>\n      <td>-0.060</td>\n      <td>0.767</td>\n      <td>-0.196</td>\n      <td>0.359</td>\n      <td>0.080</td>\n      <td>-0.956</td>\n      <td>0.857</td>\n      <td>-0.655</td>\n      <td>-0.090</td>\n      <td>-0.008</td>\n      <td>-0.596</td>\n      <td>-0.413</td>\n      <td>-1.030</td>\n      <td>0.173</td>\n      <td>-0.969</td>\n      <td>0.998</td>\n      <td>0.079</td>\n      <td>0.790</td>\n      <td>-0.776</td>\n      <td>-0.374</td>\n      <td>-1.995</td>\n      <td>0.572</td>\n      <td>0.542</td>\n      <td>0.547</td>\n      <td>0.307</td>\n      <td>-0.074</td>\n      <td>1.703</td>\n      <td>-0.003</td>\n      <td>...</td>\n      <td>-1.029</td>\n      <td>-0.340</td>\n      <td>0.052</td>\n      <td>2.122</td>\n      <td>-0.136</td>\n      <td>-1.799</td>\n      <td>1.450</td>\n      <td>1.866</td>\n      <td>-0.273</td>\n      <td>-0.237</td>\n      <td>-0.207</td>\n      <td>-0.196</td>\n      <td>-1.106</td>\n      <td>-1.560</td>\n      <td>-0.934</td>\n      <td>2.167</td>\n      <td>0.323</td>\n      <td>0.583</td>\n      <td>1.480</td>\n      <td>-0.685</td>\n      <td>-0.473</td>\n      <td>-1.066</td>\n      <td>-0.271</td>\n      <td>0.506</td>\n      <td>-0.753</td>\n      <td>1.048</td>\n      <td>-0.450</td>\n      <td>-0.300</td>\n      <td>-1.221</td>\n      <td>0.235</td>\n      <td>-0.336</td>\n      <td>-0.787</td>\n      <td>0.255</td>\n      <td>-0.031</td>\n      <td>-0.836</td>\n      <td>0.916</td>\n      <td>2.411</td>\n      <td>1.053</td>\n      <td>-1.601</td>\n      <td>-1.529</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>254</td>\n      <td>0.754</td>\n      <td>-0.245</td>\n      <td>1.173</td>\n      <td>-1.623</td>\n      <td>0.009</td>\n      <td>0.370</td>\n      <td>0.781</td>\n      <td>-1.763</td>\n      <td>-1.432</td>\n      <td>-0.930</td>\n      <td>-0.098</td>\n      <td>0.896</td>\n      <td>0.293</td>\n      <td>-0.259</td>\n      <td>0.030</td>\n      <td>-0.661</td>\n      <td>0.921</td>\n      <td>0.006</td>\n      <td>-0.631</td>\n      <td>1.284</td>\n      <td>-1.167</td>\n      <td>-0.744</td>\n      <td>-2.184</td>\n      <td>2.146</td>\n      <td>1.130</td>\n      <td>0.017</td>\n      <td>1.421</td>\n      <td>-0.590</td>\n      <td>1.938</td>\n      <td>-0.194</td>\n      <td>0.794</td>\n      <td>0.579</td>\n      <td>0.521</td>\n      <td>0.635</td>\n      <td>-0.023</td>\n      <td>-0.892</td>\n      <td>-0.363</td>\n      <td>-0.360</td>\n      <td>0.405</td>\n      <td>...</td>\n      <td>-0.486</td>\n      <td>-0.068</td>\n      <td>-0.534</td>\n      <td>-1.322</td>\n      <td>0.500</td>\n      <td>0.263</td>\n      <td>-0.745</td>\n      <td>0.578</td>\n      <td>-0.064</td>\n      <td>0.738</td>\n      <td>-0.280</td>\n      <td>0.745</td>\n      <td>-0.588</td>\n      <td>-0.429</td>\n      <td>-0.588</td>\n      <td>0.154</td>\n      <td>-1.187</td>\n      <td>1.681</td>\n      <td>-0.832</td>\n      <td>-0.437</td>\n      <td>-0.038</td>\n      <td>-1.096</td>\n      <td>-0.156</td>\n      <td>3.565</td>\n      <td>-0.428</td>\n      <td>-0.384</td>\n      <td>1.243</td>\n      <td>-0.966</td>\n      <td>1.525</td>\n      <td>0.458</td>\n      <td>2.184</td>\n      <td>-1.090</td>\n      <td>0.216</td>\n      <td>1.186</td>\n      <td>-0.143</td>\n      <td>0.322</td>\n      <td>-0.068</td>\n      <td>-0.156</td>\n      <td>-1.153</td>\n      <td>0.825</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = df_test[features].values","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_preds = []\nfor mod, score in best_models:\n    y_preds.append(mod.predict(X_test))\ny_preds = np.concatenate(y_preds, axis=1)\ny_preds.shape","execution_count":17,"outputs":[{"output_type":"execute_result","execution_count":17,"data":{"text/plain":"(19750, 10)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"subs = pd.read_csv('../input/sample_submission.csv')\nmean_preds = y_preds.mean(axis=1)\nsubs['target'] = mean_preds\nsubs.to_csv('submission.csv', index=False)","execution_count":18,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}